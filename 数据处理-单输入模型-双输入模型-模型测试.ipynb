{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, SimpleRNN, Dense, GRU, LSTM, Bidirectional, Dropout\n",
    "from keras.metrics import AUC\n",
    "from keras.models import Model\n",
    "from keras import layers\n",
    "from keras import Input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据集：\n",
    "\n",
    "data.csv 原始文件,记录细胞轨迹数据 \\\n",
    "id_link.csv 原始文件，记录细胞重叠事件 \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入原始数据并保存为csv格式\n",
    "data = pd.read_csv('data/xyarr2D191211_001.txt',sep = r' +', header=None, engine='python')\n",
    "data.columns = ['size','x','y','t','id','ovality']\n",
    "data = data[['id','t','x','y','size','ovality']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 查看轨迹数据的基本信息\n",
    "\n",
    "def tra_describe(data):\n",
    "    \n",
    "    # data格式: clos :  id t x y size ovality   index无意义\n",
    "    \n",
    "    # num_id_frame :  每个 time_frame 中的 id 数\n",
    "    # id_steps: id对应的步长\n",
    "    # steps_dis: index是步长  ‘count’是该步长的id数 ‘cum_sum’是步长从大到小累计百分比\n",
    "    # walk_range : 细胞的移动范围，有x方向、y方向\n",
    "    \n",
    "\n",
    "    # 查看每个 time_frame 中的 id 数\n",
    "    num_id_frame = data[['t','id']].groupby(by = 't').count()\n",
    "    num_id_frame.columns= ['num_id_frame']    \n",
    "\n",
    "    #查看步长分布\n",
    "    id_steps = data['id'].value_counts().to_frame() #id_count:  index是id 列为id对应的步数'steps'\n",
    "    id_steps.columns= ['steps']\n",
    "\n",
    "    steps_count = id_steps['steps'].value_counts().to_frame().sort_index(ascending=False) #index是步长，‘count’是步长出现的次数 \n",
    "    steps_count.columns= ['count']\n",
    "    steps_count['cum_sum'] = steps_count.cumsum()/steps_count.sum() #注意这里步长是倒序，方便计算cum_sum\n",
    "    steps_dis = steps_count.sort_index()\n",
    "    \n",
    "    \n",
    "    # 查看x的范围和y的范围分布  # 应该算路径而不是位移？\n",
    "\n",
    "    walk_range = data.groupby(by = 'id').apply(max)[['x','y']] - data.groupby(by = 'id').apply(min)[['x','y']]\n",
    "    walk_range['max_x_y'] = walk_range.max(axis = 1)\n",
    "    walk_range['x_y'] = np.sqrt(walk_range['x']**2 + walk_range['y']**2)\n",
    "    \n",
    "    return num_id_frame, id_steps, steps_dis, walk_range\n",
    "\n",
    "\n",
    "# clean data\n",
    "\n",
    "# ！！！！需要有 data,id_steps,walk_range 三个数据表和 min_steps ,min_walk_range 两个参数 ！！！！\n",
    "\n",
    "def get_data_clean(data,id_steps,walk_range,min_steps = 60 ,min_walk_range = 20) : \n",
    "\n",
    "    # min_steps 去除步长过小的细胞\n",
    "    # min_walk_range 去除运动范围很小的细胞\n",
    "    \n",
    "    #满足条件的id\n",
    "    \n",
    "    id_total = list(data['id'].unique())\n",
    "    \n",
    "    id_filter = id_steps[id_steps['steps'] >= min_steps].join(\n",
    "            walk_range[walk_range['max_x_y'] >= min_walk_range],  # 'max_x_y'可以换成“x_y”\n",
    "            how = 'inner')\n",
    "    \n",
    "    data_clean = data.set_index('id').loc[id_filter.index].reset_index()\n",
    "    \n",
    "    print(\"设置的最小步长为: \", min_steps, \";  最小运动范围为: \", min_walk_range)\n",
    "    print(\"被选出的id数量: \", len(id_filter))\n",
    "    print(\"占总id数量的比例: \", len(id_filter)/len(id_total))\n",
    "    print('已得到data_clean')\n",
    "    \n",
    "    return data_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_id_frame, id_steps, steps_dis, walk_range = tra_describe(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 查看移动范围分布\n",
    "plt.figure(figsize=(8,5))\n",
    "sns.distplot(walk_range['max_x_y'],bins = 100)\n",
    "plt.title('distribution of walk_range')\n",
    "plt.xlabel('walk_range')\n",
    "plt.ylabel('proportion')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 查看步长分布\n",
    "plt.figure(figsize=(8,5))\n",
    "sns.distplot(id_steps,bins = 50)\n",
    "plt.title('distribution of steps')\n",
    "plt.xlabel('steps')\n",
    "plt.ylabel('proportion')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 根据步长和移动范围分布确定截断点，获得clean之后的data\n",
    "data_clean = get_data_clean(data,id_steps,walk_range,min_steps = 60 ,min_walk_range = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入id_link数据\n",
    "id_link = pd.read_csv('data/id_link191211_001.csv', header=None)\n",
    "id_link.columns = ['id1','id2','id3','id4','type','id5','id6','id7','id8','t']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 重构 id_link 的格式，将一次完整的重叠又分裂的行为放在一行，去除只有重合/只有分裂的事件\n",
    "\n",
    "#重叠\n",
    "type0 = id_link[id_link['type'] == 0][['id1','id2','id3','id4','id5','t']].rename(columns={'id5' : 'id_overlap',\n",
    "                                                                                           't':'t1'}) \n",
    "#分开           \n",
    "type1 = id_link[id_link['type'] == 1][['id1','id5','id6','id7','id8','t']].rename(columns={'id1' : 'id_overlap',\n",
    "                                                                                           't':'t2'}) \n",
    "#拼合\n",
    "id_link_new = type0.join(type1.set_index('id_overlap'), on='id_overlap', how='inner') # how = 'outer'即可保留所有事件\n",
    "id_link_new['overlap_time'] = id_link_new['t1']-id_link_new['t2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 查看重叠时间长度的分布\n",
    "plt.figure(figsize=(8,5))\n",
    "sns.distplot(id_link_new['overlap_time'],bins =100)\n",
    "plt.title('distribution of overlap_time')\n",
    "plt.xlabel('overlap_time')\n",
    "plt.ylabel('proportion')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 根据data clean的结果过滤一些未被选择的id\n",
    "id_list = list(data_clean['id'].unique())\n",
    "a = id_link_new[id_link_new['id1'].isin(id_list)]\n",
    "b = a[a['id2'].isin(id_list)]\n",
    "c = b[b['id5'].isin(id_list)]\n",
    "id_link_filter = c[c['id6'].isin(id_list)].reset_index(drop = True)\n",
    "\n",
    "print('原数据集发生完整的重合又分开事件的个数: ',len(id_link_new))\n",
    "print('这些事件中前后四个id都在data_clean中的个数: ', len(id_link_filter))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据集 总结：\n",
    "\n",
    "data  原始文件 \\\n",
    "data_clean  去掉轨迹过短和移动范围过少的细胞  \n",
    "\n",
    "id_link  原始文件 \\\n",
    "id_link_new  重构，完整的重叠又分开 \\\n",
    "id_link_filter 筛选过的id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 单输入model \n",
    "\n",
    "\n",
    "#### 1)选取一个id连续轨迹的后30步作为碰撞前的轨迹 tra_pre\n",
    "#### 2)选取一个id连续轨迹的前30步作为碰撞后的轨迹 tra_post\n",
    "\n",
    "#### 3)将tra_pre，tra_post连接起来:\n",
    "\n",
    "   #### 若这两段轨迹来自同一cell，则输出label=1；\n",
    "   #### 若这两段轨迹来自不同cell，则输出label=0；\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取轨迹数据，并去掉overlap期间的轨迹记录（物理运动规律可能会变化？？）\n",
    "id_overlap_list = id_link_new['id_overlap'].to_list()\n",
    "df = data_clean[ ~ data_clean.id.isin(id_overlap_list)]\n",
    "id_list = df['id'].unique().tolist() #id列表\n",
    "id_t0 = df.sort_values(['id','t']).groupby('id').head(1) #每个时间桢新出现的id\n",
    "ts = list(id_t0['t'].unique()) #时间帧"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 取出前30和后30步\n",
    "# 注意!!! 使用head 和tail 时要保证数据按照时间排序 !!!!!!\n",
    "steps = 30\n",
    "\n",
    "df_post1 = df.sort_values(['id','t']).groupby('id').head(steps).set_index('id')  # 碰撞之后\n",
    "df_pre1 = df.sort_values(['id','t']).groupby('id').tail(steps).set_index('id') # 碰撞之前\n",
    "\n",
    "# 将前30的第一步的坐标移动到（0，0） 初始时间为0\n",
    "# 将后30的最后一步移动到 （0，0） 结束时间为0\n",
    "df_post2 = df_post1.sub(df_post1.groupby(by = 'id').head(1))  # sub 会对准index\n",
    "df_pre2 = df_pre1.sub(df_pre1.groupby(by = 'id').tail(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 构造label为1的数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_post3 = pd.concat([df_post2[['t','x','y']],df_post1[['size','ovality']]],axis = 1)\n",
    "df_pre3 = pd.concat([df_pre2[['t','x','y']],df_pre1[['size','ovality']]],axis = 1)\n",
    "\n",
    "# label = 1  将同一个id 的后30和前30拼接到一起 （注意：前后颠倒）\n",
    "# 并去除 post中重复的（0，0）坐标\n",
    "df_label1 = pd.concat([df_pre3,df_post3.groupby(by = 'id').tail(steps - 1)])\n",
    "df1 = df_label1.reset_index().set_index(['id','t']).sort_index()   #注意要sort_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 构造label为0的数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "li = []\n",
    "\n",
    "# 选择一些时间frame中的id 确保这些id一定代表不同的细胞\n",
    "\n",
    "for t in ts:    # 如果不用t0会有很多重复的ID，因为同一个ID在不同的 t frame 中出现！！！！！\n",
    "    \n",
    "    id_list_t = id_t0[id_t0['t'] == t]['id'].to_list()  #t frame 中的那些细胞\n",
    "    id_list_t.sort()\n",
    "    \n",
    "    i = 0\n",
    "    while i < len(id_list_t)-1 :\n",
    "        \n",
    "        id1 = id_list_t[i]\n",
    "        id2 = id_list_t[i+1]     #也和现实情况很像，因为一般发生overlap的细胞编号一般是连着的\n",
    "        # id3 = id_list_t[i+2]\n",
    "\n",
    "        tra = pd.concat([df_pre2.loc[id1],df_post2.loc[id2].tail(steps - 1)])\n",
    "        others = pd.concat([df_pre1.loc[id1],df_post1.loc[id2].tail(steps - 1)])\n",
    "        \n",
    "        li.append(pd.concat([tra[['t','x','y']],others[['size','ovality']]],axis = 1))\n",
    "        i = i+1  # 或者可以i+2利用更多id\n",
    "\n",
    "df0_concat = pd.concat(li,keys = range(len(li)),names=['sample'])\n",
    "df0 = df0_concat.reset_index().set_index (['sample','t'])[['x','y','size','ovality']].sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义函数： input 张量化并划分训练集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Tensorize(df) :  # 输入需要张量化的dataframe, df.index=['id','t'] df.values = ['features']\n",
    "    # 样本个数\n",
    "    num_sample = df.index.unique(0).shape[0]\n",
    "    # 时间步长\n",
    "    timesteps = df.index.unique(1).shape[0]\n",
    "    # 特征数量\n",
    "    num_features = df.shape[1]\n",
    "\n",
    "    #张量化\n",
    "    ar = df.to_numpy().reshape((num_sample, timesteps, num_features)) #(sample, timesteps, features)\n",
    "    \n",
    "    print('shape of array is : ', ar.shape)\n",
    "    \n",
    "    return ar\n",
    "\n",
    "def get_dataset_for_model(df0,df1):\n",
    "    \n",
    "    ar0 = Tensorize(df0)\n",
    "    # label each sample with 0\n",
    "    label0 = np.zeros(ar0.shape[0])\n",
    "\n",
    "    ar1 = Tensorize(df1)\n",
    "    # label each sample with 1\n",
    "    label1 = np.ones(ar1.shape[0])\n",
    "\n",
    "    ar = np.concatenate((ar1, ar0), axis = 0)\n",
    "    label = np.concatenate((label1, label0), axis = 0)\n",
    "    \n",
    "    return ar,label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义函数： 训练模型  （以3层Bi-LSTM为例）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_single(input_train,label_train,epochs=4):\n",
    "    \n",
    "    #build model\n",
    "    model = Sequential()\n",
    "    model.add(\n",
    "        Bidirectional(LSTM(64, return_sequences=True), input_shape=(None, input_train.shape[-1]))\n",
    "    )\n",
    "    model.add(\n",
    "        Bidirectional(LSTM(64, return_sequences=True))\n",
    "    )\n",
    "    model.add(Bidirectional(LSTM(32)))\n",
    "\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    \n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    #model.summary()\n",
    "    \n",
    "    \n",
    "    # train model\n",
    "    model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "    history= model.fit(input_train, label_train, epochs=epochs, batch_size=64, validation_split=0.2)\n",
    "    \n",
    "    return model,history\n",
    "\n",
    "\n",
    "def get_training_result(history):\n",
    "# 绘制结果\n",
    "\n",
    "    acc = history.history['acc']\n",
    "    val_acc = history.history['val_acc']\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "\n",
    "    epochs = range(1, len(acc) + 1)\n",
    "\n",
    "    fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(15, 5))\n",
    "\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "    plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "    plt.title('Training and validation accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "    plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "    plt.title('Training and validation loss')\n",
    "    plt.legend()\n",
    "    \n",
    "def get_test_result(model,input_test,label_test):\n",
    "    \n",
    "    # evaluate on test dataset\n",
    "    print(\"Evaluate on test data\")\n",
    "    results = model.evaluate(input_test, label_test, batch_size=128)\n",
    "    print(\"test loss, test acc:\", results)\n",
    "\n",
    "\n",
    "    # 比较预测和真实的label\n",
    "    test_result = pd.concat([pd.DataFrame(model.predict(input_test)),pd.DataFrame(label_test)],axis = 1)\n",
    "    test_result.columns = ['predict','label']\n",
    "    test_result = test_result.sort_values('predict').reset_index(drop = True)\n",
    "\n",
    "    # 画图\n",
    "    #plt.figure(figsize=(15, 10))\n",
    "    #plt.scatter(test_result.reset_index()['index'],test_result['label'],s=0.2,c='red')\n",
    "    #plt.scatter(test_result.reset_index()['index'],test_result['predict'],s=0.3,c='blue')    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 选择特征输入，以(x,y,size)为例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ar,label = get_dataset_for_model(df0[['x','y','size']],df1[['x','y','size']])\n",
    "input_train, input_test, label_train, label_test = train_test_split(ar,label,test_size=0.2, random_state=420)\n",
    "model4,history4 = train_model_single(input_train4,label_train4,epochs = 5)\n",
    "get_training_result(history)\n",
    "get_test_result(model4,input_test4,label_test4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 双输入模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 取出前30和后30步\n",
    "# 注意!!! 使用head 和tail 时要保证数据按照时间排序\n",
    "\n",
    "steps = 30\n",
    "\n",
    "df_post1 = df.sort_values(['id','t']).groupby('id').head(steps).set_index('id')  # 碰撞之后\n",
    "df_pre1 = df.sort_values(['id','t']).groupby('id').tail(steps).set_index('id') # 碰撞之前\n",
    "\n",
    "# 将新的步长为30的轨迹起点移动到（0，0） 初始时间为0  \n",
    "# 和单输入模型有区别\n",
    "df_post2 = df_post1.sub(df_post1.groupby(by = 'id').head(1))  \n",
    "df_pre2 = df_pre1.sub(df_pre1.groupby(by = 'id').head(1))\n",
    "\n",
    "df_post3 = pd.concat([df_post2[['t','x','y']],df_post1[['size','ovality']]],axis = 1)\n",
    "df_pre3 = pd.concat([df_pre2[['t','x','y']],df_pre1[['size','ovality']]],axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 准备label = 0 数据集\n",
    "input1_label0 = df_pre1.head(0)\n",
    "input2_label0 = df_post1.head(0)\n",
    "\n",
    "for t in ts:\n",
    "    id_list_t = id_t0[id_t0['t'] == t]['id'].to_list()  #t frame 中的那些细胞\n",
    "    id_list_t.sort()\n",
    "    num_id = len (id_list_t)\n",
    "    \n",
    "    #错位一下\n",
    "    df_pre_0 = df_pre3.loc[id_list_t[:num_id-1]] \n",
    "    df_post_0 = df_post3.loc[id_list_t[1:]]\n",
    "    \n",
    "    input1_label0 = input1_label0.append(df_pre_0)\n",
    "    input2_label0 = input2_label0.append(df_post_0)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 统一格式\n",
    "\n",
    "input1_label0 = input1_label0.reset_index().set_index(['id','t'])\n",
    "input2_label0 = input2_label0.reset_index().set_index(['id','t'])\n",
    "\n",
    "input1_label1 = df_pre3.reset_index().set_index(['id','t'])\n",
    "input2_label1 = df_post3.reset_index().set_index(['id','t'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义函数：生成输入张量、训练集和测试集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Tensorize(df) :  # 输入需要张量化的dataframe, df.index=['id','t'] df.values = ['features']\n",
    "    # 样本个数\n",
    "    num_sample = df.index.unique(0).shape[0]\n",
    "    # 时间步长\n",
    "    timesteps = df.index.unique(1).shape[0]\n",
    "    # 特征数量\n",
    "    num_features = df.shape[1]\n",
    "\n",
    "    #张量化\n",
    "    ar = df.to_numpy().reshape((num_sample, timesteps, num_features)) #(sample, timesteps, features)\n",
    "    \n",
    "    print('shape of array is : ', ar.shape)\n",
    "    \n",
    "    return ar\n",
    "\n",
    "def split(df,test_size=0.2):\n",
    "    \n",
    "    ar = Tensorize(df)\n",
    "    num = ar.shape[0]\n",
    "    test_num = int(np.floor(num * test_size))\n",
    "    \n",
    "    ar_train = ar[test_num:]\n",
    "    ar_test = ar[:test_num]\n",
    "    \n",
    "    return ar_train,ar_test\n",
    "\n",
    "\n",
    "def get_data_for_model(input1_label1,input2_label1,input1_label0,input2_label0,features):\n",
    "\n",
    "    #从label1数据中抽样\n",
    "\n",
    "    # split training and test dataset\n",
    "    # 不能随机打乱，因为input1 和 input2 有严格的对应关系    \n",
    "\n",
    "    input1_l1_train, input1_l1_test = split(input1_label1[features])\n",
    "    input2_l1_train, input2_l1_test = split(input2_label1[features])\n",
    "\n",
    "    input1_l0_train, input1_l0_test = split(input1_label0[features])\n",
    "    input2_l0_train, input2_l0_test = split(input2_label0[features])\n",
    "\n",
    "    input1_train = np.concatenate( (input1_l1_train, input1_l0_train), axis = 0)\n",
    "    input2_train = np.concatenate( (input2_l1_train, input2_l0_train), axis = 0)\n",
    "    label_train = np.concatenate( (np.ones(input1_l1_train.shape[0]), np.zeros(input1_l0_train.shape[0]) ), axis = 0)\n",
    "\n",
    "    input1_test = np.concatenate(  (input1_l1_test, input1_l0_test), axis = 0)\n",
    "    input2_test = np.concatenate(  (input2_l1_test, input2_l0_test), axis = 0)\n",
    "    label_test = np.concatenate( (np.ones(input1_l1_test.shape[0]), np.zeros(input1_l0_test.shape[0]) ), axis = 0)\n",
    "\n",
    "    return input1_train,input2_train,label_train,input1_test,input2_test,label_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 搭建模型（以两层Bi-LSTM为例）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_model(input1_train, input2_train, label_train, num_features, epochs=6):\n",
    "\n",
    "    input1 = Input(shape=(None,num_features), dtype='float64', name='input1') # timesteps 长度可变\n",
    "    lstm11 = Bidirectional(LSTM(128, return_sequences=True))(input1)\n",
    "    lstm12 = Bidirectional(LSTM(128))(lstm11)\n",
    "\n",
    "    input2 = Input(shape=(None,num_features), dtype='float64', name='input2')\n",
    "    lstm21 = Bidirectional(LSTM(128, return_sequences=True))(input2)\n",
    "    lstm22 = Bidirectional(LSTM(128))(lstm21)\n",
    "\n",
    "\n",
    "    x = layers.concatenate([lstm12, lstm22])\n",
    "\n",
    "    # 堆叠多个全连接网络层\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    x = Dense(32, activation='relu')(x)\n",
    "\n",
    "    # 最后添加主要的逻辑回归层\n",
    "    output = Dense(1, activation='sigmoid', name='output')(x)\n",
    "\n",
    "    model = Model([input1, input2], output)\n",
    "    #model.summary()\n",
    "    \n",
    "    #model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=[AUC(name='auc')])\n",
    "    model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "    history = model.fit([input1_train, input2_train], label_train, epochs=epochs, batch_size=64)\n",
    "    \n",
    "    return model,history\n",
    "\n",
    "\n",
    "def get_training_result(history):\n",
    "# 绘制结果\n",
    "\n",
    "    acc = history.history['acc']\n",
    "    val_acc = history.history['val_acc']\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "\n",
    "    epochs = range(1, len(acc) + 1)\n",
    "\n",
    "    fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(15, 5))\n",
    "\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "    plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "    plt.title('Training and validation accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "    plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "    plt.title('Training and validation loss')\n",
    "    plt.legend()\n",
    "\n",
    "def get_test_result(model,input1_test,input2_test,label_test):\n",
    "    \n",
    "    # evaluate on test dataset\n",
    "    print(\"Evaluate on test data\")\n",
    "    results = model.evaluate([input1_test,input2_test],label_test, batch_size=64)\n",
    "    print(\"test loss, test acc:\", results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练模型：以输入（x,y,size)为例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input1_train_c,input2_train_c,label_train_c,input1_test_c,input2_test_c,label_test_c = get_data_for_model(\n",
    "    input1_label1,input2_label1,input1_label0,input2_label0,\n",
    "    features = ['x','y','size'])\n",
    "\n",
    "model_c,history_c = train_model(input1_train_c, input2_train_c, label_train_c, num_features = 3,epochs = 6)\n",
    "get_test_result(model_c,input1_test_c, input2_test_c,label_test_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 在真实碰撞情况中测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构造需要判断的轨迹input\n",
    "\n",
    "li = []\n",
    "\n",
    "for i in range(len(id_link_filter)):\n",
    "    \n",
    "\n",
    "    #获取id\n",
    "    id1 = id_link_filter.loc[i,'id1']\n",
    "    id2 = id_link_filter.loc[i,'id2']\n",
    "    id5 = id_link_filter.loc[i,'id5']\n",
    "    id6 = id_link_filter.loc[i,'id6']\n",
    "    \n",
    "    df = data_clean\n",
    "    \n",
    "\n",
    "    #获取轨迹\n",
    "    tra1 = df[df['id'] == id1].set_index('id').sort_values('t')\n",
    "    tra2 = df[df['id'] == id2].set_index('id').sort_values('t')\n",
    "\n",
    "    tra5 = df[df['id'] == id5].set_index('id').sort_values('t')\n",
    "    tra6 = df[df['id'] == id6].set_index('id').sort_values('t')\n",
    "\n",
    "    #构造input\n",
    "    t1 = pd.concat([tra1.sub(tra1.tail(1))[['t','x','y']],tra1[['size','ovality']]],axis = 1).tail(30)\n",
    "    t2 = pd.concat([tra2.sub(tra2.tail(1))[['t','x','y']],tra2[['size','ovality']]],axis = 1).tail(30)\n",
    "    t5 = pd.concat([tra5.sub(tra5.head(1))[['t','x','y']],tra5[['size','ovality']]],axis = 1).head(30).tail(29)\n",
    "    t6 = pd.concat([tra6.sub(tra6.head(1))[['t','x','y']],tra6[['size','ovality']]],axis = 1).head(30).tail(29)\n",
    "\n",
    "    \n",
    "\n",
    "    input1 = pd.concat([t1,t5])\n",
    "    input2 = pd.concat([t1,t6])\n",
    "    input3 = pd.concat([t2,t5])\n",
    "    input4 = pd.concat([t2,t6])\n",
    "    \n",
    "    li.append(input1)\n",
    "    li.append(input2)\n",
    "    li.append(input3)\n",
    "    li.append(input4)\n",
    "\n",
    "#整合data\n",
    "#x_concat = pd.concat([input1,input2,input3,input4],keys = range(4),names=['sample'])\n",
    "\n",
    "x_concat = pd.concat(li,keys = range(len(li)),names=['sample'])  \n",
    "    \n",
    "x_test = x_concat.reset_index().set_index (['sample','t'])[['x','y','size','ovality']].sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 输入模型\n",
    "x_test2 = Tensorize(x_test[['x','y','size']])\n",
    "predict = pd.DataFrame(model.predict(x_test2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#按重叠事件整理\n",
    "# 模型概率值\n",
    "final_result = pd.concat([predict.iloc[0::4].reset_index(drop = True),predict.iloc[1::4].reset_index(drop = True),\n",
    "               predict.iloc[2::4].reset_index(drop = True),predict.iloc[3::4].reset_index(drop = True)],\n",
    "               axis = 1)\n",
    "final_result.columns = ['1','3','4','2'] # ['15','16','25','26']\n",
    "final_result = final_result[['1','2','3','4']]\n",
    "final_result.columns = ['tra1','tra2','tra3','tra4']\n",
    "\n",
    "# 0，1分类 thredhold = 0.5\n",
    "\n",
    "predict_01 = predict.copy()\n",
    "predict_01.columns = ['value']\n",
    "predict_01.loc[predict_01['value'] > 0.5] = 1\n",
    "predict_01.loc[predict_01['value'] <= 0.5] = 0\n",
    "\n",
    "final_result_01 = pd.concat([predict_01.iloc[0::4].reset_index(drop = True),predict_01.iloc[1::4].reset_index(drop = True),\n",
    "               predict_01.iloc[2::4].reset_index(drop = True),predict_01.iloc[3::4].reset_index(drop = True)],\n",
    "               axis = 1)\n",
    "final_result_01.columns = ['1','3','4','2']\n",
    "final_result_01 = final_result_01[['1','2','3','4']]\n",
    "final_result_01.columns = ['tra1','tra2','tra3','tra4']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_result_01.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_result.head(10).round(2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
